{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c586abc0",
   "metadata": {},
   "source": [
    "# Site Data Integrity Triage & Remediation (Company-Agnostic)\n",
    "\n",
    "This notebook is a **data integrity workflow** designed to:\n",
    "- Identify *missing* and *inconsistent* records in a site inventory dataset\n",
    "- Reconcile those gaps by searching other available datasets (where applicable)\n",
    "- Produce a **prioritized work queue** for field/operations teams to validate and update data that cannot be reliably inferred\n",
    "\n",
    "> This is a refactored, sanitized version of an operational “cleanup notebook.”  \n",
    "> It is intentionally structured to be easy to follow and reuse with different datasets.\n",
    "\n",
    "## Core idea\n",
    "\n",
    "Automation is used to:\n",
    "1. **Detect** where the data is incomplete or inconsistent  \n",
    "2. **Locate** candidate values from other sources when possible  \n",
    "3. **Route** unresolved items to humans (e.g., field techs) with clear, targeted work instructions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d397436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration (edit these) ---\n",
    "from pathlib import Path\n",
    "\n",
    "DATA_DIR   = Path(\"./data\")\n",
    "OUTPUT_DIR = Path(\"./output\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Primary dataset (site inventory)\n",
    "SITE_INVENTORY_FILE = DATA_DIR / \"site_inventory.xlsx\"   # or .csv\n",
    "\n",
    "# Optional supporting datasets (examples)\n",
    "SUPPORTING_FILES = {\n",
    "    \"access_hours\": DATA_DIR / \"access_hours.xlsx\",\n",
    "    \"meter_numbers\": DATA_DIR / \"meter_numbers.xlsx\",\n",
    "    \"small_cell_inventory\": DATA_DIR / \"small_cell_inventory.xlsx\",\n",
    "    \"assignments\": DATA_DIR / \"assignments.xlsx\",\n",
    "}\n",
    "\n",
    "# Column mapping (rename to match your data)\n",
    "COLUMN_MAP = {\n",
    "    \"site_id\": \"SITE_ID\",\n",
    "    \"market\": \"MARKET\",\n",
    "    \"site_type\": \"SITE_TYPE\",\n",
    "    \"address\": \"ADDRESS\",\n",
    "    \"city\": \"CITY\",\n",
    "    \"state\": \"STATE\",\n",
    "    \"zip\": \"ZIP\",\n",
    "    # examples of integrity-critical fields:\n",
    "    \"access_hours\": \"ACCESS_HOURS\",\n",
    "    \"meter_number\": \"METER_NUMBER\",\n",
    "    \"power_system\": \"POWER_SYSTEM\",\n",
    "    \"fop_assignment\": \"FOPS_ASSIGNMENT\",\n",
    "    \"lat\": \"LAT\",\n",
    "    \"lon\": \"LON\",\n",
    "}\n",
    "\n",
    "# Integrity rules: required fields and rule-specific thresholds\n",
    "REQUIRED_FIELDS = [\"site_id\", \"state\"]\n",
    "GEO_REQUIRED_FIELDS = [\"lat\", \"lon\"]   # if your site inventory is expected to have GPS built in\n",
    "\n",
    "# Work queue settings\n",
    "MAX_WORK_ITEMS_PER_CATEGORY = 500\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a2f68d",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcaff3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 200)\n",
    "pd.set_option(\"display.width\", 160)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb3b289",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350f3c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- IO helpers ---\n",
    "def read_table(path: str | 'Path') -> pd.DataFrame:\n",
    "    path = str(path)\n",
    "    if path.lower().endswith(\".csv\"):\n",
    "        return pd.read_csv(path)\n",
    "    if path.lower().endswith(\".parquet\"):\n",
    "        return pd.read_parquet(path)\n",
    "    # default to excel\n",
    "    return pd.read_excel(path)\n",
    "\n",
    "def standardize_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    out.columns = [str(c).strip() for c in out.columns]\n",
    "    return out\n",
    "\n",
    "def get_col(name: str) -> str:\n",
    "    \"\"\"Resolve logical column name via COLUMN_MAP.\"\"\"\n",
    "    if name not in COLUMN_MAP:\n",
    "        raise KeyError(f\"COLUMN_MAP missing key: {name}\")\n",
    "    return COLUMN_MAP[name]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f479ad29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Data integrity helpers ---\n",
    "def missing_summary(df: pd.DataFrame, logical_cols: list[str]) -> pd.DataFrame:\n",
    "    cols = [get_col(c) for c in logical_cols if get_col(c) in df.columns]\n",
    "    out = pd.DataFrame({\n",
    "        \"column\": cols,\n",
    "        \"missing_count\": [df[c].isna().sum() for c in cols],\n",
    "        \"missing_pct\": [df[c].isna().mean() for c in cols],\n",
    "    }).sort_values(\"missing_count\", ascending=False)\n",
    "    return out\n",
    "\n",
    "def find_missing_required(df: pd.DataFrame, required_logical_cols: list[str]) -> pd.DataFrame:\n",
    "    cols = [get_col(c) for c in required_logical_cols]\n",
    "    missing_mask = False\n",
    "    for c in cols:\n",
    "        if c in df.columns:\n",
    "            missing_mask = missing_mask | df[c].isna() | (df[c].astype(str).str.strip() == \"\")\n",
    "    return df.loc[missing_mask].copy()\n",
    "\n",
    "def find_duplicates(df: pd.DataFrame, key_logical_col: str) -> pd.DataFrame:\n",
    "    key = get_col(key_logical_col)\n",
    "    if key not in df.columns:\n",
    "        return pd.DataFrame()\n",
    "    return df[df.duplicated(subset=[key], keep=False)].sort_values(key)\n",
    "\n",
    "def enforce_types(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    # Example: coerce lat/lon to numeric if present\n",
    "    for k in [\"lat\", \"lon\"]:\n",
    "        col = get_col(k)\n",
    "        if col in out.columns:\n",
    "            out[col] = pd.to_numeric(out[col], errors=\"coerce\")\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6d6fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Reconciliation utilities ---\n",
    "def left_join_fill(primary: pd.DataFrame, secondary: pd.DataFrame, on: list[str], fill_map: dict[str, str]) -> pd.DataFrame:\n",
    "    \"\"\"Join secondary onto primary and fill missing primary values from secondary.\n",
    "\n",
    "    fill_map maps: primary_col -> secondary_col\n",
    "    \"\"\"\n",
    "    merged = primary.merge(secondary, on=on, how=\"left\", suffixes=(\"\", \"_src\"))\n",
    "    out = merged.copy()\n",
    "    for primary_col, secondary_col in fill_map.items():\n",
    "        if primary_col in out.columns and secondary_col in out.columns:\n",
    "            out[primary_col] = out[primary_col].where(out[primary_col].notna() & (out[primary_col].astype(str).str.strip() != \"\"), out[secondary_col])\n",
    "    # drop helper cols from secondary (optional)\n",
    "    drop_cols = [c for c in out.columns if c.endswith(\"_src\")]\n",
    "    if drop_cols:\n",
    "        out = out.drop(columns=drop_cols, errors=\"ignore\")\n",
    "    return out\n",
    "\n",
    "def build_work_queue(df: pd.DataFrame, category: str, reason: str, site_id_col: str, extra_cols: list[str] | None = None) -> pd.DataFrame:\n",
    "    \"\"\"Produce a minimal work queue for operations/field validation.\"\"\"\n",
    "    out = pd.DataFrame({\n",
    "        \"category\": category,\n",
    "        \"site_id\": df[site_id_col].astype(str),\n",
    "        \"reason\": reason,\n",
    "    })\n",
    "    if extra_cols:\n",
    "        for c in extra_cols:\n",
    "            if c in df.columns:\n",
    "                out[c] = df[c]\n",
    "    return out.drop_duplicates().head(MAX_WORK_ITEMS_PER_CATEGORY)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f94846",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b98b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load primary dataset ---\n",
    "site_df = standardize_columns(read_table(SITE_INVENTORY_FILE))\n",
    "site_df = enforce_types(site_df)\n",
    "\n",
    "site_id_col = get_col(\"site_id\")\n",
    "print(\"Rows:\", len(site_df))\n",
    "site_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4cf071",
   "metadata": {},
   "source": [
    "## Audit integrity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1db86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Integrity audit (quick) ---\n",
    "print(\"Missing required fields:\")\n",
    "display(missing_summary(site_df, REQUIRED_FIELDS))\n",
    "\n",
    "print(\"\\nMissing GPS fields (if expected):\")\n",
    "display(missing_summary(site_df, GEO_REQUIRED_FIELDS))\n",
    "\n",
    "print(\"\\nDuplicate site IDs:\")\n",
    "dupes = find_duplicates(site_df, \"site_id\")\n",
    "display(dupes.head(25))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff3df5d",
   "metadata": {},
   "source": [
    "## Identify issue categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4791ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Identify issue categories ---\n",
    "issues = {}\n",
    "\n",
    "missing_required = find_missing_required(site_df, REQUIRED_FIELDS)\n",
    "issues[\"missing_required\"] = missing_required\n",
    "\n",
    "missing_geo = find_missing_required(site_df, GEO_REQUIRED_FIELDS)\n",
    "issues[\"missing_geo\"] = missing_geo\n",
    "\n",
    "access_col = get_col(\"access_hours\")\n",
    "issues[\"missing_access_hours\"] = (\n",
    "    site_df[site_df[access_col].isna() | (site_df[access_col].astype(str).str.strip()==\"\")].copy()\n",
    "    if access_col in site_df.columns else pd.DataFrame()\n",
    ")\n",
    "\n",
    "meter_col = get_col(\"meter_number\")\n",
    "issues[\"missing_meter_number\"] = (\n",
    "    site_df[site_df[meter_col].isna() | (site_df[meter_col].astype(str).str.strip()==\"\")].copy()\n",
    "    if meter_col in site_df.columns else pd.DataFrame()\n",
    ")\n",
    "\n",
    "power_col = get_col(\"power_system\")\n",
    "issues[\"unknown_power_system\"] = (\n",
    "    site_df[site_df[power_col].isna() | (site_df[power_col].astype(str).str.strip()==\"\")].copy()\n",
    "    if power_col in site_df.columns else pd.DataFrame()\n",
    ")\n",
    "\n",
    "pd.Series({k: len(v) for k, v in issues.items()}).sort_values(ascending=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f74d7c3",
   "metadata": {},
   "source": [
    "## Reconcile from supporting sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad52a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- OPTIONAL: Reconcile from supporting datasets (fill what can be safely filled) ---\n",
    "site_reconciled = site_df.copy()\n",
    "\n",
    "# Fill access hours from a supporting file (if present)\n",
    "if SUPPORTING_FILES.get(\"access_hours\") and SUPPORTING_FILES[\"access_hours\"].exists():\n",
    "    access_df = standardize_columns(read_table(SUPPORTING_FILES[\"access_hours\"]))\n",
    "    # Ensure site_id column name aligns\n",
    "    if site_id_col not in access_df.columns:\n",
    "        print(f\"WARNING: access_hours file missing key column: {site_id_col}\")\n",
    "    else:\n",
    "        site_reconciled = left_join_fill(\n",
    "            site_reconciled,\n",
    "            access_df,\n",
    "            on=[site_id_col],\n",
    "            fill_map={get_col(\"access_hours\"): get_col(\"access_hours\")}\n",
    "        )\n",
    "\n",
    "# Fill meter number from a supporting file (if present)\n",
    "if SUPPORTING_FILES.get(\"meter_numbers\") and SUPPORTING_FILES[\"meter_numbers\"].exists():\n",
    "    meter_df = standardize_columns(read_table(SUPPORTING_FILES[\"meter_numbers\"]))\n",
    "    if site_id_col not in meter_df.columns:\n",
    "        print(f\"WARNING: meter_numbers file missing key column: {site_id_col}\")\n",
    "    else:\n",
    "        site_reconciled = left_join_fill(\n",
    "            site_reconciled,\n",
    "            meter_df,\n",
    "            on=[site_id_col],\n",
    "            fill_map={get_col(\"meter_number\"): get_col(\"meter_number\")}\n",
    "        )\n",
    "\n",
    "# Compare issue counts after reconciliation\n",
    "def issue_counts(df: pd.DataFrame) -> pd.Series:\n",
    "    out = {}\n",
    "    if access_col in df.columns:\n",
    "        out[\"missing_access_hours\"] = int((df[access_col].isna() | (df[access_col].astype(str).str.strip()==\"\")).sum())\n",
    "    if meter_col in df.columns:\n",
    "        out[\"missing_meter_number\"] = int((df[meter_col].isna() | (df[meter_col].astype(str).str.strip()==\"\")).sum())\n",
    "    if power_col in df.columns:\n",
    "        out[\"unknown_power_system\"] = int((df[power_col].isna() | (df[power_col].astype(str).str.strip()==\"\")).sum())\n",
    "    return pd.Series(out).sort_values(ascending=False)\n",
    "\n",
    "print(\"Before reconciliation:\")\n",
    "display(issue_counts(site_df))\n",
    "\n",
    "print(\"After reconciliation:\")\n",
    "display(issue_counts(site_reconciled))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103199aa",
   "metadata": {},
   "source": [
    "## Create technician / operations work queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ebdfb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Generate work queues (for humans) ---\n",
    "work_items = []\n",
    "\n",
    "# Power system (example of a field that typically needs technician verification)\n",
    "if power_col in site_reconciled.columns:\n",
    "    unknown_power = site_reconciled[site_reconciled[power_col].isna() | (site_reconciled[power_col].astype(str).str.strip()==\"\")].copy()\n",
    "    if len(unknown_power):\n",
    "        work_items.append(\n",
    "            build_work_queue(\n",
    "                unknown_power,\n",
    "                category=\"field_verification\",\n",
    "                reason=\"Power system must be verified by technician and entered in the system of record.\",\n",
    "                site_id_col=site_id_col,\n",
    "                extra_cols=[get_col(\"market\"), get_col(\"site_type\"), get_col(\"address\"), get_col(\"city\"), get_col(\"state\"), get_col(\"zip\")]\n",
    "            )\n",
    "        )\n",
    "\n",
    "# Missing GPS (if expected) should be fixed at the source\n",
    "lat_col, lon_col = get_col(\"lat\"), get_col(\"lon\")\n",
    "if lat_col in site_reconciled.columns and lon_col in site_reconciled.columns:\n",
    "    missing_geo2 = site_reconciled[site_reconciled[lat_col].isna() | site_reconciled[lon_col].isna()].copy()\n",
    "    if len(missing_geo2):\n",
    "        work_items.append(\n",
    "            build_work_queue(\n",
    "                missing_geo2,\n",
    "                category=\"data_fix\",\n",
    "                reason=\"Missing LAT/LON. Verify GPS and update the system of record.\",\n",
    "                site_id_col=site_id_col,\n",
    "                extra_cols=[get_col(\"market\"), get_col(\"site_type\"), get_col(\"address\"), get_col(\"city\"), get_col(\"state\"), get_col(\"zip\")]\n",
    "            )\n",
    "        )\n",
    "\n",
    "work_queue_df = pd.concat(work_items, ignore_index=True) if work_items else pd.DataFrame(columns=[\"category\",\"site_id\",\"reason\"])\n",
    "work_queue_df.head(50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a3c88d",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ecffda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Export outputs ---\n",
    "reconciled_out = OUTPUT_DIR / \"site_inventory_reconciled.xlsx\"\n",
    "workqueue_out  = OUTPUT_DIR / \"work_queue.xlsx\"\n",
    "audit_out      = OUTPUT_DIR / \"audit_summary.xlsx\"\n",
    "\n",
    "site_reconciled.to_excel(reconciled_out, index=False)\n",
    "work_queue_df.to_excel(workqueue_out, index=False)\n",
    "\n",
    "with pd.ExcelWriter(audit_out) as writer:\n",
    "    missing_summary(site_df, REQUIRED_FIELDS + GEO_REQUIRED_FIELDS + [\"access_hours\",\"meter_number\",\"power_system\"]).to_excel(writer, sheet_name=\"missing_summary\", index=False)\n",
    "    dupes.to_excel(writer, sheet_name=\"duplicate_site_ids\", index=False)\n",
    "\n",
    "print(\"Wrote:\", reconciled_out)\n",
    "print(\"Wrote:\", workqueue_out)\n",
    "print(\"Wrote:\", audit_out)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
